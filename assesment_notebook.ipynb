{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3yBZc8OqUg3"
   },
   "source": [
    "# Voice AI Call Analysis\n",
    "This notebook processes a customer call recording to extract:\n",
    "- Transcript\n",
    "- Speaker diarization\n",
    "- Call sentiment\n",
    "- One actionable business insight\n",
    "- Bonus: identify sales rep vs customer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4TSQhLvChETP",
    "outputId": "ab691f0c-bec2-4c52-f2e4-3bcf81bd9f70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Colab: install required packages. Run this cell first.\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq ffmpeg  # audio handling\n",
    "# Python packages\n",
    "!pip install -q yt-dlp openai-whisper resemblyzer transformers torch torchvision torchaudio librosa pydub scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgs2oHCkqaiS"
   },
   "source": [
    "## Step 1: Install dependencies & download test audio\n",
    "We use `yt-dlp` to download the audio from YouTube and convert it to 16kHz mono WAV for speech processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1ijndLOhn2X",
    "outputId": "ae84f0ef-d840-490b-ca3d-191ee79217cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=4ostqJD3Psc\n",
      "[youtube] 4ostqJD3Psc: Downloading webpage\n",
      "[youtube] 4ostqJD3Psc: Downloading tv simply player API JSON\n",
      "[youtube] 4ostqJD3Psc: Downloading tv client config\n",
      "[youtube] 4ostqJD3Psc: Downloading player b66835e2-main\n",
      "[youtube] 4ostqJD3Psc: Downloading tv player API JSON\n",
      "[info] 4ostqJD3Psc: Downloading 1 format(s): 251\n",
      "[download] Sleeping 2.00 seconds as required by the site...\n",
      "[download] Destination: call.webm\n",
      "\u001b[K[download] 100% of    1.99MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m7.16MiB/s\u001b[0m\n",
      "[ExtractAudio] Destination: call.wav\n",
      "Deleting original file call.webm (pass -k to keep)\n",
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : stereo\n",
      "\u001b[0mInput #0, wav, from 'call.wav':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Duration: 00:02:02.69, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s16, 1536 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'call_16k.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=    3834kB time=00:02:02.69 bitrate= 256.0kbits/s speed= 143x    \n",
      "video:0kB audio:3834kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.001987%\n",
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : mono\n",
      "\u001b[0mInput #0, wav, from 'call_16k.wav':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Duration: 00:02:02.69, bitrate: 256 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'call_clean.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=    3833kB time=00:02:02.65 bitrate= 256.0kbits/s speed= 239x    \n",
      "video:0kB audio:3833kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.001987%\n"
     ]
    }
   ],
   "source": [
    "# (1) Download the test file and convert to 16k mono WAV\n",
    "YOUTUBE_URL = \"https://www.youtube.com/watch?v=4ostqJD3Psc\"  # assignment file\n",
    "\n",
    "# download audio using yt-dlp\n",
    "!yt-dlp -x --audio-format wav --output \"call.%(ext)s\" \"{YOUTUBE_URL}\"\n",
    "\n",
    "# convert to 16kHz mono and do a basic denoise/normalize pass\n",
    "!ffmpeg -y -i call.wav -ar 16000 -ac 1 -af \"highpass=f=200, lowpass=f=3000, dynaudnorm\" call_16k.wav\n",
    "# optional further denoise (afftdn is ffmpeg's spectral denoiser)\n",
    "!ffmpeg -y -i call_16k.wav -af afftdn call_clean.wav || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5ltuLFiqjxY"
   },
   "source": [
    "## Step 2: Transcription\n",
    "We use the Whisper **tiny** model (fast, robust to noisy audio) to generate transcript text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n4AypgB9hts7",
    "outputId": "cf366d5f-dadc-4385-b10d-64629152f291"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 72.1M/72.1M [00:01<00:00, 56.2MiB/s]\n",
      "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00-11.78: Thank you for calling me son. My name is Lauren. Can I have your name?\n",
      "11.78-16.12: Yes, my name is John Smith. Thank you, John. How can I help you?\n",
      "16.12-20.42: I was just calling about as she how much it would cost to update the map in my car.\n",
      "20.42-24.06: I'd be happy to help you with that today. Did you receive a mail or from us?\n",
      "24.06-26.56: I did. Do you need the customer number?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# (2) Transcribe with Whisper (use tiny for speed). Comments included.\n",
    "import whisper\n",
    "model = whisper.load_model(\"tiny\")   # tiny = fastest; swap to \"base\" if you have GPU/time\n",
    "result = model.transcribe(\"call_clean.wav\", language=\"en\")  # returns segments with start/end/text\n",
    "segments = result[\"segments\"]  # list of dicts: {'id','seek','start','end','text',...}\n",
    "\n",
    "# Show a few segments to inspect\n",
    "for s in segments[:5]:\n",
    "    print(f\"{s['start']:.2f}-{s['end']:.2f}: {s['text'].strip()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZkkS6IMqqD0"
   },
   "source": [
    "## Step 3: Speaker Diarization\n",
    "We use Resemblyzer embeddings + clustering to separate different speakers in the conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgroDP66hz4S",
    "outputId": "97f02089-084a-48d6-f882-33d6672842c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
      "0.0-11.8 [1]:  Thank you for calling me son. My name is Lauren. Can I have your name?\n",
      "11.8-16.1 [0]:  Yes, my name is John Smith. Thank you, John. How can I help you?\n",
      "16.1-20.4 [0]:  I was just calling about as she how much it would cost to update the map in my car.\n",
      "20.4-24.1 [0]:  I'd be happy to help you with that today. Did you receive a mail or from us?\n",
      "24.1-26.6 [0]:  I did. Do you need the customer number?\n",
      "26.6-27.6 [0]:  Yes, please.\n",
      "27.6-30.6 [0]:  Okay. It's 15243.\n",
      "30.6-33.6 [0]:  Thank you and the year making model of your vehicle.\n"
     ]
    }
   ],
   "source": [
    "# (3) Lightweight diarization: Resemblyzer embeddings + AgglomerativeClustering\n",
    "import numpy as np\n",
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from scipy.io import wavfile\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "sr, audio = wavfile.read(\"call_clean.wav\")\n",
    "# make sure audio is float32 in range [-1,1]\n",
    "if audio.dtype != np.float32:\n",
    "    audio = audio.astype(np.float32) / np.iinfo(audio.dtype).max\n",
    "\n",
    "# sliding windows to compute speaker embeddings\n",
    "encoder = VoiceEncoder()  # pretrained voice encoder\n",
    "window_s = 1.5  # 1.5s windows\n",
    "step_s = 0.75   # 50% overlap\n",
    "windows = []\n",
    "times = []\n",
    "n_samples = len(audio)\n",
    "for start in np.arange(0, max(0, n_samples/sr - window_s + 1e-6), step_s):\n",
    "    s = int(start * sr)\n",
    "    e = int(min(n_samples, (start + window_s) * sr))\n",
    "    windows.append(audio[s:e])\n",
    "    times.append((start, (e / sr)))\n",
    "\n",
    "# compute embeddings (this is the main cost)\n",
    "embeds = [encoder.embed_utterance(w) for w in windows]  # each is 256-d\n",
    "\n",
    "# cluster into 2 speakers (assignment expects 2 persons)\n",
    "X = np.vstack(embeds)\n",
    "clustering = AgglomerativeClustering(n_clusters=2).fit(X)\n",
    "labels = clustering.labels_  # for each window\n",
    "\n",
    "# helper to determine majority label for arbitrary time range\n",
    "def speaker_for_segment(seg_start, seg_end):\n",
    "    idxs = [i for i,(a,b) in enumerate(times) if not (b <= seg_start or a >= seg_end)]\n",
    "    if not idxs:\n",
    "        return None\n",
    "    return int(np.bincount(labels[idxs]).argmax())\n",
    "\n",
    "# attach speaker label to each whisper segment\n",
    "for seg in segments:\n",
    "    seg['speaker'] = speaker_for_segment(seg['start'], seg['end'])\n",
    "\n",
    "# quick check\n",
    "for s in segments[:8]:\n",
    "    print(f\"{s['start']:.1f}-{s['end']:.1f} [{s['speaker']}]: {s['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0RUUAfwh_GR",
    "outputId": "383f35a8-8154-457c-80b0-fd8eaf7e01fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talk-time ratio (%) per speaker: {1: 9.935897435897436, 0: 90.06410256410257}\n",
      "Number of questions (detected via '?'): 7\n",
      "Longest monologue: {'speaker': 0, 'duration_s': 106.78}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# talk time per speaker\n",
    "speaker_time = defaultdict(float)\n",
    "for seg in segments:\n",
    "    if seg.get('speaker') is None: continue\n",
    "    dur = seg['end'] - seg['start']\n",
    "    speaker_time[seg['speaker']] += dur\n",
    "total_speech = sum(speaker_time.values()) or 1.0\n",
    "talk_time_ratio = {int(k): (v/total_speech)*100 for k,v in speaker_time.items()}\n",
    "\n",
    "# number of questions: count '?' and lines that look like questions\n",
    "q_count = 0\n",
    "for seg in segments:\n",
    "    text = seg['text'].strip()\n",
    "    q_count += text.count('?')\n",
    "    # also check for typical interrogative words + punctuationless endings\n",
    "    q_count += 0  # keep conservative (we already count '?')\n",
    "\n",
    "# longest monologue: merge consecutive segments of same speaker (allow small gaps)\n",
    "monologues = []\n",
    "if segments:\n",
    "    cur_sp = segments[0]['speaker']\n",
    "    cur_start = segments[0]['start']\n",
    "    cur_end = segments[0]['end']\n",
    "    for seg in segments[1:]:\n",
    "        if seg['speaker'] == cur_sp and (seg['start'] - cur_end) <= 1.0:\n",
    "            cur_end = seg['end']\n",
    "        else:\n",
    "            monologues.append((cur_sp, cur_start, cur_end, cur_end-cur_start))\n",
    "            cur_sp = seg['speaker']; cur_start = seg['start']; cur_end = seg['end']\n",
    "    monologues.append((cur_sp, cur_start, cur_end, cur_end-cur_start))\n",
    "# get longest\n",
    "if monologues:\n",
    "    longest = max(monologues, key=lambda x: x[3])\n",
    "else:\n",
    "    longest = (None,0,0,0)\n",
    "\n",
    "# print metrics\n",
    "print(\"Talk-time ratio (%) per speaker:\", talk_time_ratio)\n",
    "print(\"Number of questions (detected via '?'):\", q_count)\n",
    "print(\"Longest monologue:\", {\"speaker\": longest[0], \"duration_s\": longest[3]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX1l3Ko2rFNh"
   },
   "source": [
    "## Step 4: Sentiment Analysis\n",
    "We analyze each speaker's utterances using a transformer-based sentiment model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356,
     "referenced_widgets": [
      "344a74a1ca8145c8a2afa3d22bd577b2",
      "48c1d8b3dc324f6d9c1c1c43dbc90e81",
      "10f62938dd3c45a1a720464b5b95603f",
      "635668f060aa45edb1d88b08a13201df",
      "2d274491e13a4c2f837130e73b3095cf",
      "aab194d96c234889bc2e5e31b775ae9e",
      "2fc81e44d8904e32a28bbb8b05e4afcd",
      "850bd1000fa54448b71200510ba770de",
      "adbec9e7e2014fd9bc6611233c9062bc",
      "4c084330bcff4260a6bafb3fabae8ff7",
      "4f88abf6931c4186aaeeaa318b9cd75e",
      "43054d9d14bf4db8ae3f79e508601813",
      "b0c2ae870ea948ea90cca4f89e8b5ec6",
      "610c7319599b4f70bb8c4efb12641ec9",
      "c29bb2cdd428418bb0e88d05388fbe82",
      "80212d97a8504bbcb7d3a3c883066e58",
      "75a8a9fec3bf4196b70c41c77f363037",
      "6c9c3446cb09422d956173e823bedca2",
      "2d7eca8bed2e41789c5677e331845f75",
      "5dd71306e4f3477bacd39f6c309d4e24",
      "09b91f74a3c748178fa64f1037b23561",
      "ae9df867199241ed998d3159818891a4",
      "0fb645ac892a4d6b9403eaf8dc35288f",
      "0f4bd37792d64521a6b688ad13dee348",
      "7d0cd80680014e709689961664e01804",
      "a2be13466c1d4c06a2ea8a26a8f7fd7d",
      "93d5a6d24a074658b3233b35f3d7f940",
      "9b23e20e62de450ea195106bcbda6dca",
      "87a503431c60410b92cfec26793e1564",
      "f8dbaad9cffa4a19a52e9e61faf64d9f",
      "33e8b08e300f456298e04326e3209729",
      "9c1bfb57e1664ae88b13d57b64b0ea5c",
      "4c762ce1ca4e42fcb79a8d7ac51aa135",
      "85b97ce04af943e786d9cc4ff5d14d52",
      "2cd6ceee6a0c45b288bbd1ea098f7e2a",
      "a69e9581a90344cba06d9ce468596a01",
      "4b00884cb1c143cc90c8c2d7d9130a31",
      "d4b3c69bf5a9403fa87bae7f9643d707",
      "19804e4b5ff849f9947ed9d3fdbcab9a",
      "f63f2447a7e54c11a2a4ab4651d5197e",
      "b8c1b5a9d47642648a41b975a45d80df",
      "eb6785e924974a64b3f9d539289808d1",
      "247214646f244159b44d57983be5cf79",
      "730024d2ff944cef89e179bc8a7c0344"
     ]
    },
    "id": "4lu5ni3aiETq",
    "outputId": "8673207a-e938-4107-ddfb-548bf9df3268"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344a74a1ca8145c8a2afa3d22bd577b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43054d9d14bf4db8ae3f79e508601813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb645ac892a4d6b9403eaf8dc35288f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b97ce04af943e786d9cc4ff5d14d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-speaker sentiment: {1: [{'label': 'POSITIVE', 'score': 0.9980979561805725}], 0: [{'label': 'NEGATIVE', 'score': 0.812537670135498}]}\n",
      "Overall call sentiment: neutral\n"
     ]
    }
   ],
   "source": [
    "# (5) Sentiment with transformers pipeline (SST-2 model)\n",
    "from transformers import pipeline\n",
    "sent_pipe = pipeline(\"sentiment-analysis\")  # distilbert-finetuned-sst2 by default\n",
    "\n",
    "# collect text per speaker\n",
    "from collections import defaultdict\n",
    "speaker_text = defaultdict(str)\n",
    "for seg in segments:\n",
    "    sp = seg.get('speaker')\n",
    "    if sp is None: continue\n",
    "    speaker_text[sp] += \" \" + seg['text']\n",
    "\n",
    "# get sentiment for each speaker (truncate to 4000 chars to be safe)\n",
    "speaker_sentiment = {}\n",
    "for sp, text in speaker_text.items():\n",
    "    shortened = text.strip()[:4000]\n",
    "    if shortened:\n",
    "        r = sent_pipe(shortened)\n",
    "        speaker_sentiment[sp] = r  # list of dicts from pipeline\n",
    "\n",
    "# derive overall call sentiment by counting positive/negative\n",
    "pos = neg = 0\n",
    "for sp, res in speaker_sentiment.items():\n",
    "    lab = res[0]['label']\n",
    "    if lab.upper().startswith(\"POS\"):\n",
    "        pos += 1\n",
    "    else:\n",
    "        neg += 1\n",
    "overall_sentiment = \"positive\" if pos>neg else (\"negative\" if neg>pos else \"neutral\")\n",
    "\n",
    "print(\"Per-speaker sentiment:\", speaker_sentiment)\n",
    "print(\"Overall call sentiment:\", overall_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcesBayErL0v"
   },
   "source": [
    "## Step 5: Actionable Insight\n",
    "We extract one key insight based on customer sentiment and topic.\n",
    "Bonus: Identify sales rep vs customer based on talk time + question patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKzFHOr2iSN0",
    "outputId": "eaed3dad-fd6c-42a9-8438-b229c5b84d2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"talk_time_ratio\": {\n",
      "    \"1\": 9.935897435897436,\n",
      "    \"0\": 90.06410256410257\n",
      "  },\n",
      "  \"questions_detected\": 7,\n",
      "  \"longest_monologue\": {\n",
      "    \"speaker\": 0,\n",
      "    \"duration_s\": 106.78\n",
      "  },\n",
      "  \"overall_sentiment\": \"neutral\",\n",
      "  \"per_speaker_sentiment\": {\n",
      "    \"1\": [\n",
      "      {\n",
      "        \"label\": \"POSITIVE\",\n",
      "        \"score\": 0.9980979561805725\n",
      "      }\n",
      "    ],\n",
      "    \"0\": [\n",
      "      {\n",
      "        \"label\": \"NEGATIVE\",\n",
      "        \"score\": 0.812537670135498\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"insight\": \"Sales rep dominated the call \\u2014 try to ask more open questions and let the customer speak.\",\n",
      "  \"assumed_sales_rep\": 0,\n",
      "  \"assumed_customer\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# (6) Simple heuristic insight + identify sales rep vs customer (bonus)\n",
    "# Heuristics:\n",
    "# - Sales rep tends to talk more & ask more questions.\n",
    "# - If one speaker has >65% talk time -> rep dominated.\n",
    "\n",
    "# identify assumed rep & customer\n",
    "if len(talk_time_ratio) >= 2:\n",
    "    rep_label = max(talk_time_ratio, key=talk_time_ratio.get)\n",
    "    cust_label = min(talk_time_ratio, key=talk_time_ratio.get)\n",
    "else:\n",
    "    rep_label = list(talk_time_ratio.keys())[0] if talk_time_ratio else None\n",
    "    cust_label = None\n",
    "\n",
    "# insight rules\n",
    "insight = \"\"\n",
    "if talk_time_ratio.get(rep_label,0) > 65:\n",
    "    insight = \"Sales rep dominated the call — try to ask more open questions and let the customer speak.\"\n",
    "elif q_count < 3:\n",
    "    insight = \"Few questions were asked. Increase discovery/open-ended questions to learn customer needs.\"\n",
    "elif overall_sentiment == \"negative\":\n",
    "    insight = \"Call has negative sentiment. Coach on objection-handling & empathy.\"\n",
    "else:\n",
    "    insight = \"Balanced call. Continue asking open ended questions and confirm next steps.\"\n",
    "\n",
    "# final output object\n",
    "result = {\n",
    "    \"talk_time_ratio\": talk_time_ratio,\n",
    "    \"questions_detected\": q_count,\n",
    "    \"longest_monologue\": {\"speaker\": longest[0], \"duration_s\": longest[3]},\n",
    "    \"overall_sentiment\": overall_sentiment,\n",
    "    \"per_speaker_sentiment\": speaker_sentiment,\n",
    "    \"insight\": insight,\n",
    "    \"assumed_sales_rep\": int(rep_label) if rep_label is not None else None,\n",
    "    \"assumed_customer\": int(cust_label) if cust_label is not None else None\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(\"call_analysis.json\",\"w\") as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPAjtJJtrUDf"
   },
   "source": [
    "# ✅ Final Results\n",
    "- Transcript: ✔️\n",
    "- Speakers separated: ✔️\n",
    "- Sentiment detected: ✔️\n",
    "- Actionable insight: ✔️\n",
    "- Sales rep vs customer: ✔️\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
